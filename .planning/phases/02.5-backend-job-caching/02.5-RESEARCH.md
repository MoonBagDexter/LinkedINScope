# Phase 2.5: Backend Job Caching - Research

**Researched:** 2026-02-02
**Domain:** Backend job caching with Supabase Edge Functions and pg_cron scheduling
**Confidence:** HIGH

## Summary

The task is to move job fetching from per-user client-side JSearch API calls to a backend service that periodically caches jobs in Supabase. Research confirms **Supabase Edge Functions with pg_cron scheduling** is the optimal free-tier solution. Edge Functions run on a Deno runtime (TypeScript-first, uses built-in `fetch` API), scheduled via PostgreSQL's `pg_cron` extension combined with `pg_net` for HTTP invocation. The free tier provides 500,000 Edge Function invocations per month with 150s timeout and 256MB memory—more than sufficient for daily job caching.

**Key architectural insight:** Separate the "fast" user interface from "slow" external API calls using the job queue pattern. Backend service fetches JSearch API once per schedule, stores in database, frontend queries cached data with <100ms response times.

**Primary recommendation:** Use Supabase Edge Function triggered by pg_cron daily (preserves 93% of monthly JSearch quota for manual triggers), employ upsert with `onConflict` to preserve engagement data, mark stale jobs inactive via boolean flag rather than deletion.

## Standard Stack

The established libraries/tools for backend job caching with Supabase:

### Core
| Library | Version | Purpose | Why Standard |
|---------|---------|---------|--------------|
| Supabase Edge Functions | Deno runtime | Serverless function execution | Native Supabase integration, 500K free invocations/month, 0ms cold start |
| pg_cron | PostgreSQL ext | Job scheduling | Built into Supabase free tier, SQL-native, no external scheduler needed |
| pg_net | PostgreSQL ext | HTTP client for DB | Enables pg_cron to invoke Edge Functions, included in Supabase |
| Deno fetch API | Built-in | HTTP requests | Deno's native API, no dependencies required (axios incompatible with Deno) |
| @supabase/supabase-js | 2.93.3+ | Database client | Already in project, works in Edge Functions with service role key |

### Supporting
| Library | Version | Purpose | When to Use |
|---------|---------|---------|-------------|
| Supabase Vault | Built-in | Secure secret storage | Store project URL and API keys for pg_cron invocations |
| exponential-backoff | 3.x (npm) | Retry logic | Graceful handling of JSearch API rate limits and transient failures |

### Alternatives Considered
| Instead of | Could Use | Tradeoff |
|------------|-----------|----------|
| Edge Function | Node.js on Vercel | Vercel requires payment, more deployment complexity, overkill for prototype |
| Edge Function | GitHub Actions cron | Requires repo setup, less integrated, harder to debug, can't easily trigger manually |
| Daily cron | 6-hour cron | Uses 4x quota (120 vs 30 monthly calls), minimal freshness benefit for service jobs |
| Upsert | Delete + Insert | Loses engagement data (click_count, lane), breaks user experience |

**Installation:**

Edge Functions are deployed via Supabase CLI:
```bash
# Already installed: @supabase/supabase-js
npm install -g supabase  # CLI for function deployment

# Create Edge Function
supabase functions new job-sync

# Deploy
supabase functions deploy job-sync
```

## Architecture Patterns

### Recommended Project Structure
```
supabase/
├── functions/
│   └── job-sync/           # Edge Function
│       └── index.ts        # Main handler
└── migrations/
    └── YYYYMMDD_job_caching_schema.sql  # DB schema updates
src/
├── services/
│   └── jobCache.ts         # Frontend queries cached jobs
└── hooks/
    └── useCachedJobs.ts    # React Query hook for cache
```

### Pattern 1: Scheduled Edge Function via pg_cron + pg_net
**What:** PostgreSQL cron job uses pg_net to invoke Edge Function on schedule
**When to use:** Need recurring backend tasks on Supabase free tier
**Example:**
```sql
-- Source: https://supabase.com/docs/guides/functions/schedule-functions
select
  cron.schedule(
    'job-sync-daily',
    '0 6 * * *',  -- 6 AM UTC daily
    $$
    select
      net.http_post(
          url:= (select decrypted_secret from vault.decrypted_secrets where name = 'project_url') || '/functions/v1/job-sync',
          headers:=jsonb_build_object(
            'Content-type', 'application/json',
            'Authorization', 'Bearer ' || (select decrypted_secret from vault.decrypted_secrets where name = 'service_role_key')
          ),
          body:=jsonb_build_object('scheduled', true, 'timestamp', now())::text
      ) as request_id;
    $$
  );
```

**Critical detail:** Use `service_role_key` from Vault for authentication—bypasses RLS and provides full DB access. Store secrets in Vault, not hardcoded.

### Pattern 2: Edge Function with Service Role Client
**What:** Edge Function creates Supabase client with service role key for privileged operations
**When to use:** Edge Function needs to bypass RLS policies to write/update data
**Example:**
```typescript
// Source: https://github.com/orgs/supabase/discussions/15631
import { createClient } from '@supabase/supabase-js';

const supabase = createClient(
  Deno.env.get('SUPABASE_URL')!,
  Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!  // Bypasses RLS
);

// Can now write to any table regardless of RLS policies
await supabase.from('jobs').upsert(jobData);
```

### Pattern 3: Upsert with Preserved Columns
**What:** Merge new job data while preserving engagement columns (click_count, lane)
**When to use:** Updating cached data without losing user interaction state
**Example:**
```typescript
// Source: https://supabase.com/docs/reference/javascript/upsert
await supabase
  .from('jobs')
  .upsert(
    jobsData.map(job => ({
      job_id: job.job_id,
      job_title: job.job_title,
      // ... other JSearch fields
      // NOTE: Do NOT include click_count or lane - upsert preserves existing values
      is_active: true,
      last_seen: new Date().toISOString()
    })),
    {
      onConflict: 'job_id',  // Primary key or unique constraint
      ignoreDuplicates: false  // Update existing rows
    }
  );
```

**Key insight:** PostgreSQL upsert (`INSERT ... ON CONFLICT ... DO UPDATE`) only updates columns included in the data payload. Omitting `click_count` and `lane` preserves existing values.

### Pattern 4: Exponential Backoff for External API Calls
**What:** Retry failed API calls with exponentially increasing delays plus jitter
**When to use:** Calling external APIs with rate limits (JSearch: 500/month, rate limit errors possible)
**Example:**
```typescript
// Source: https://v-checha.medium.com/advanced-node-js-patterns-implementing-robust-retry-logic-656cf70f8ee9
async function fetchWithRetry(url: string, options: RequestInit, maxRetries = 3) {
  for (let attempt = 1; attempt <= maxRetries; attempt++) {
    try {
      const response = await fetch(url, options);

      // Retry on rate limit or server errors
      if (response.status === 429 || response.status >= 500) {
        if (attempt === maxRetries) throw new Error(`API error: ${response.status}`);

        // Exponential backoff: 10ms, 20ms, 40ms + jitter
        const delay = Math.min(1000, 10 * Math.pow(2, attempt - 1));
        const jitter = Math.random() * 0.3 * delay;  // ±30% jitter
        await new Promise(resolve => setTimeout(resolve, delay + jitter));
        continue;
      }

      if (!response.ok) throw new Error(`API error: ${response.status}`);
      return response;
    } catch (error) {
      if (attempt === maxRetries) throw error;
      // Retry on network errors
      const delay = Math.min(1000, 10 * Math.pow(2, attempt - 1));
      await new Promise(resolve => setTimeout(resolve, delay));
    }
  }
}
```

**Best practices:**
- Only retry retriable errors (429, 500-599, network failures)
- Add jitter to prevent thundering herd
- Cap max delay to avoid excessive waits
- Wait AFTER failure, not before first attempt

### Pattern 5: Stale Job Management with Boolean Flag
**What:** Mark jobs as inactive when no longer in API rather than deleting them
**When to use:** Preserving engagement history while preventing stale data from appearing
**Example:**
```typescript
// Mark all jobs inactive, then set active=true for current jobs
// This approach handles removed jobs automatically
await supabase.from('jobs').update({ is_active: false }).eq('is_active', true);

await supabase.from('jobs').upsert(
  currentJobs.map(job => ({ ...job, is_active: true })),
  { onConflict: 'job_id' }
);

// Frontend query only shows active jobs
const { data } = await supabase
  .from('jobs')
  .select('*')
  .eq('is_active', true);
```

### Anti-Patterns to Avoid
- **Hardcoding credentials in SQL**: Use Supabase Vault for project_url and service_role_key
- **Using anon_key for cron jobs**: Can't bypass RLS, may hit permission errors. Use service_role_key.
- **Waiting before first API call**: Adds unnecessary latency even on success. Only wait before retries.
- **Deleting stale jobs**: Loses engagement history (clicks, lane data). Use is_active flag instead.
- **Using axios in Edge Functions**: Axios is incompatible with Deno. Use built-in `fetch` API.
- **Minute-level cron without cleanup**: `cron.job_run_details` grows large. Daily jobs avoid this issue.

## Don't Hand-Roll

Problems that look simple but have existing solutions:

| Problem | Don't Build | Use Instead | Why |
|---------|-------------|-------------|-----|
| Scheduled tasks | Custom Node.js setInterval service | pg_cron + Edge Functions | Free tier included, SQL-native, no server maintenance, survives restarts |
| API retry logic | Manual try/catch with setTimeout | exponential-backoff npm package OR manual with jitter | Handles edge cases (max retries, jitter, retriable errors), battle-tested |
| Secure credential storage | Environment variables in code | Supabase Vault with decrypted_secrets | Encrypted at rest, accessible in DB context, audit trail |
| Database upsert | SELECT then UPDATE or INSERT | PostgreSQL upsert with onConflict | Atomic operation, no race conditions, preserves omitted columns |
| Stale data removal | DELETE statements | Boolean is_active flag | Preserves engagement history, enables analytics, reversible |

**Key insight:** Supabase free tier provides production-grade scheduling, secret management, and edge compute that would require paid services elsewhere. Leverage these instead of building custom solutions.

## Common Pitfalls

### Pitfall 1: Free Tier Project Auto-Pause After 7 Days Inactivity
**What goes wrong:** Supabase free tier pauses projects with no database activity for 7 consecutive days. Cron jobs stop running, manual resume required.
**Why it happens:** Free tier abuse prevention mechanism
**How to avoid:**
- **For 1-2 day prototype:** Not a concern, project won't reach 7 days
- **For longer-term:** Cron job itself counts as activity—daily job sync keeps project alive
- **Fallback:** GitHub Actions weekly ping (redundant for daily cron)
**Warning signs:** Project shows "Paused" in Supabase dashboard, Edge Functions return 503 errors
**Source:** [Supabase free tier inactivity documentation](https://shadhujan.medium.com/how-to-keep-supabase-free-tier-projects-active-d60fd4a17263)

### Pitfall 2: RLS Policies Block Service Role Operations
**What goes wrong:** Edge Function with service_role_key still gets RLS permission errors when querying/updating tables
**Why it happens:** Supabase client defaults to anon key behavior even when service key provided, or RLS policies explicitly reference service_role (which doesn't work)
**How to avoid:**
```typescript
// WRONG: Using environment variable anon key
const supabase = createClient(
  Deno.env.get('SUPABASE_URL')!,
  Deno.env.get('SUPABASE_ANON_KEY')!
);

// CORRECT: Explicitly use service role key
const supabase = createClient(
  Deno.env.get('SUPABASE_URL')!,
  Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!
);
```
**Warning signs:** 403 Forbidden errors, "row level security policy violation" in logs
**Source:** [Supabase RLS troubleshooting](https://supabase.com/docs/guides/troubleshooting/why-is-my-service-role-key-client-getting-rls-errors-or-not-returning-data-7_1K9z)

### Pitfall 3: Local Cron + Edge Function Networking Issues
**What goes wrong:** Edge Function works locally via `supabase functions serve`, but pg_cron invocation fails with network errors
**Why it happens:** Docker networking—cron runs in DB container, needs to reach function container. `localhost` or `host.docker.internal` may not resolve correctly.
**How to avoid:**
- Test pg_cron invocation in **deployed environment** (staging project), not locally
- For local testing, use manual triggers: `supabase functions invoke job-sync --method POST`
- Accept that pg_cron + Edge Functions work better deployed than in local dev
**Warning signs:** Cron job shows success in `cron.job_run_details` but Edge Function logs show no invocation
**Source:** [Supabase cron local issues discussion](https://github.com/orgs/supabase/discussions/14747)

### Pitfall 4: JSearch API Quota Burn Without Caching Headers
**What goes wrong:** React Query refetches jobs on every mount/focus, burning through 500/month quota rapidly
**Why it happens:** Default staleTime is 0 (always refetch), especially problematic during development
**How to avoid:** Already solved in current codebase—React Query configured with 24h staleTime. Backend caching makes this unnecessary (queries local DB, not JSearch).
**Warning signs:** JSearch returns 429 Too Many Requests, unexpected quota depletion
**Verification:** Check `.planning/phases/01-foundation/01-01-PLAN.md` confirms 24h staleTime for useJobs hook

### Pitfall 5: Upsert Accidentally Overwrites Engagement Data
**What goes wrong:** Upsert sets click_count=0 and lane='new' for existing jobs, resetting user engagement
**Why it happens:** Including these columns in upsert payload with default values
**How to avoid:**
```typescript
// BAD: Includes click_count and lane
await supabase.from('jobs').upsert({
  job_id: '123',
  job_title: 'New Title',
  click_count: 0,  // ❌ Overwrites existing count!
  lane: 'new'      // ❌ Resets lane!
});

// GOOD: Omits engagement columns
await supabase.from('jobs').upsert({
  job_id: '123',
  job_title: 'New Title',
  // click_count and lane preserved automatically
  is_active: true,
  last_seen: new Date().toISOString()
});
```
**Warning signs:** Jobs randomly reset to "New" lane, click counts disappear
**PostgreSQL behavior:** `ON CONFLICT DO UPDATE SET` only updates specified columns. Omitted columns retain existing values.

### Pitfall 6: Forgetting to Set is_active on New Jobs
**What goes wrong:** New jobs inserted but remain invisible because is_active defaults to NULL/false
**Why it happens:** Schema defaults is_active to false, upsert doesn't explicitly set true
**How to avoid:** Always include `is_active: true` in upsert payload OR set schema default to true
**Warning signs:** Job count in logs doesn't match jobs displayed in UI
**Best practice:** Add schema default `is_active BOOLEAN DEFAULT TRUE` to handle edge cases

## Code Examples

Verified patterns from official sources:

### Complete Edge Function: Job Sync Handler
```typescript
// Source: Deno Edge Functions architecture
// File: supabase/functions/job-sync/index.ts
import { createClient } from 'https://esm.sh/@supabase/supabase-js@2';

interface JSearchJob {
  job_id: string;
  job_title: string;
  employer_name: string;
  job_city: string;
  job_state: string;
  job_country: string;
  job_apply_link: string;
  job_min_salary?: number;
  job_max_salary?: number;
  job_salary_currency?: string;
  job_salary_period?: string;
}

Deno.serve(async (req) => {
  try {
    // Create service role client (bypasses RLS)
    const supabase = createClient(
      Deno.env.get('SUPABASE_URL')!,
      Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!
    );

    console.log('[job-sync] Starting job fetch from JSearch API...');

    // Fetch jobs from JSearch API with retry
    const response = await fetchWithRetry(
      'https://jsearch.p.rapidapi.com/search?query=fast+food+crew+member+USA&num_pages=1',
      {
        headers: {
          'X-RapidAPI-Key': Deno.env.get('JSEARCH_API_KEY')!,
          'X-RapidAPI-Host': 'jsearch.p.rapidapi.com',
        },
      }
    );

    const { data: jobs } = await response.json();
    console.log(`[job-sync] Fetched ${jobs.length} jobs from JSearch`);

    // Mark all existing jobs as inactive (we'll reactivate current ones)
    await supabase.from('jobs').update({ is_active: false }).eq('is_active', true);

    // Upsert jobs (preserves click_count and lane for existing jobs)
    const { error: upsertError } = await supabase.from('jobs').upsert(
      jobs.map((job: JSearchJob) => ({
        job_id: job.job_id,
        job_title: job.job_title,
        employer_name: job.employer_name,
        job_city: job.job_city,
        job_state: job.job_state,
        job_country: job.job_country,
        job_apply_link: job.job_apply_link,
        job_min_salary: job.job_min_salary,
        job_max_salary: job.job_max_salary,
        job_salary_currency: job.job_salary_currency,
        job_salary_period: job.job_salary_period,
        is_active: true,
        last_seen: new Date().toISOString(),
        // NOTE: click_count and lane NOT included - preserves existing values
      })),
      { onConflict: 'job_id' }
    );

    if (upsertError) throw upsertError;

    console.log(`[job-sync] Successfully synced ${jobs.length} jobs at ${new Date().toISOString()}`);

    return new Response(
      JSON.stringify({
        success: true,
        jobCount: jobs.length,
        timestamp: new Date().toISOString(),
      }),
      { headers: { 'Content-Type': 'application/json' } }
    );
  } catch (error) {
    console.error('[job-sync] Error:', error);
    return new Response(
      JSON.stringify({ error: error.message }),
      { status: 500, headers: { 'Content-Type': 'application/json' } }
    );
  }
});

// Retry helper with exponential backoff
async function fetchWithRetry(url: string, options: RequestInit, maxRetries = 3) {
  for (let attempt = 1; attempt <= maxRetries; attempt++) {
    try {
      const response = await fetch(url, options);
      if (response.status === 429 || response.status >= 500) {
        if (attempt === maxRetries) throw new Error(`API error: ${response.status}`);
        const delay = Math.min(1000, 10 * Math.pow(2, attempt - 1));
        const jitter = Math.random() * 0.3 * delay;
        await new Promise((resolve) => setTimeout(resolve, delay + jitter));
        continue;
      }
      if (!response.ok) throw new Error(`API error: ${response.status}`);
      return response;
    } catch (error) {
      if (attempt === maxRetries) throw error;
      const delay = Math.min(1000, 10 * Math.pow(2, attempt - 1));
      await new Promise((resolve) => setTimeout(resolve, delay));
    }
  }
  throw new Error('Max retries exceeded');
}
```

### Frontend: Query Cached Jobs
```typescript
// Source: Current project pattern from useLaneJobs.ts
// File: src/hooks/useCachedJobs.ts
import { useQuery } from '@tanstack/react-query';
import { supabase } from '../services/supabase';
import type { Job } from '../types/job';

export interface JobWithEngagement extends Job {
  click_count: number;
  lane: 'new' | 'trending' | 'graduated';
}

export function useCachedJobs() {
  return useQuery({
    queryKey: ['cached-jobs'],
    queryFn: async () => {
      const { data, error } = await supabase
        .from('jobs')
        .select('*')
        .eq('is_active', true)  // Only show current jobs
        .order('last_seen', { ascending: false });

      if (error) throw error;
      return data as JobWithEngagement[];
    },
    staleTime: 1000 * 60 * 5,  // 5 minutes - data refreshed daily by backend
  });
}
```

### Database Migration: Add Caching Columns
```sql
-- Source: PostgreSQL upsert and boolean flag patterns
-- File: supabase/migrations/YYYYMMDD_add_job_caching.sql

-- Add columns to support backend caching
ALTER TABLE jobs
ADD COLUMN IF NOT EXISTS is_active BOOLEAN DEFAULT TRUE NOT NULL,
ADD COLUMN IF NOT EXISTS last_seen TIMESTAMPTZ DEFAULT NOW();

-- Create index for active job queries (performance optimization)
CREATE INDEX IF NOT EXISTS idx_jobs_is_active ON jobs(is_active);

-- Add comment for documentation
COMMENT ON COLUMN jobs.is_active IS 'False when job no longer in JSearch API but preserved for engagement history';
COMMENT ON COLUMN jobs.last_seen IS 'Last time job appeared in JSearch API refresh';
```

### Setup pg_cron Schedule
```sql
-- Source: https://supabase.com/docs/guides/functions/schedule-functions
-- Run in Supabase SQL Editor

-- First, store secrets in Vault
SELECT vault.create_secret('https://YOUR_PROJECT_REF.supabase.co', 'project_url');
SELECT vault.create_secret('YOUR_SERVICE_ROLE_KEY', 'service_role_key');

-- Schedule daily job sync at 6 AM UTC
SELECT cron.schedule(
  'job-sync-daily',
  '0 6 * * *',  -- Every day at 6 AM UTC (uses cron syntax)
  $$
  SELECT net.http_post(
    url := (SELECT decrypted_secret FROM vault.decrypted_secrets WHERE name = 'project_url') || '/functions/v1/job-sync',
    headers := jsonb_build_object(
      'Content-Type', 'application/json',
      'Authorization', 'Bearer ' || (SELECT decrypted_secret FROM vault.decrypted_secrets WHERE name = 'service_role_key')
    ),
    body := jsonb_build_object(
      'scheduled', true,
      'timestamp', NOW()
    )::text
  ) AS request_id;
  $$
);

-- View scheduled jobs
SELECT * FROM cron.job;

-- View execution history
SELECT * FROM cron.job_run_details ORDER BY start_time DESC LIMIT 10;

-- Manually trigger for testing (or use Edge Function invoke)
SELECT net.http_post(
  url := (SELECT decrypted_secret FROM vault.decrypted_secrets WHERE name = 'project_url') || '/functions/v1/job-sync',
  headers := jsonb_build_object(
    'Content-Type', 'application/json',
    'Authorization', 'Bearer ' || (SELECT decrypted_secret FROM vault.decrypted_secrets WHERE name = 'service_role_key')
  ),
  body := '{"manual": true}'
);
```

## State of the Art

| Old Approach | Current Approach | When Changed | Impact |
|--------------|------------------|--------------|--------|
| Client-side API calls per user | Backend caching with scheduled fetches | 2020-2024 (JAMstack evolution) | 90%+ reduction in API quota usage, sub-second page loads vs 3-5s |
| Custom Node.js cron services | Supabase pg_cron + Edge Functions | 2023-2024 (Supabase Cron GA) | Zero infrastructure management, free tier support, SQL-native scheduling |
| Axios for HTTP requests | Native fetch API | 2022+ (Node 18, Deno adoption) | No dependencies, standardized across runtimes, better for edge computing |
| Hard delete stale records | Soft delete with boolean flags | Ongoing best practice | Preserves analytics, enables data recovery, supports audit trails |
| Environment variables for secrets | Supabase Vault | 2024 (Vault GA) | Encrypted at rest, accessible in DB context, prevents secret leakage in logs |

**Deprecated/outdated:**
- **pg_cron with hardcoded service_role_key in SQL**: Use Vault with `decrypted_secrets` view (security improvement)
- **pgjwt extension for JWT generation in cron**: Deprecated in Postgres 17, use Vault-stored keys instead
- **axios in Edge Functions**: Incompatible with Deno runtime, use native fetch
- **Complex custom retry logic**: npm packages like `exponential-backoff` handle edge cases better

## Open Questions

Things that couldn't be fully resolved:

1. **JSearch API exact rate limiting behavior**
   - What we know: 500 requests/month free tier per official project documentation
   - What's unclear: Rate limit per-second/minute, whether burst limits exist, exact 429 retry-after headers
   - Recommendation: Implement exponential backoff as defensive measure, monitor initial deploys for rate limit errors. Daily schedule (30 req/month) provides 16x safety margin.

2. **Edge Function cold start performance with Deno.env**
   - What we know: Edge Functions have "millisecond" cold starts per Supabase docs, environment variables available
   - What's unclear: Performance impact of repeated `Deno.env.get()` calls vs caching in module scope
   - Recommendation: Follow official examples using `Deno.env.get()` inline—likely optimized by runtime. Premature optimization unnecessary.

3. **Optimal pg_cron cleanup frequency**
   - What we know: `cron.job_run_details` grows indefinitely, can slow down over time
   - What's unclear: Growth rate for daily jobs, when cleanup becomes necessary, recommended retention period
   - Recommendation: Daily jobs on 1-2 day prototype are not a concern. For longer-term, add cleanup cron after 30 days: `DELETE FROM cron.job_run_details WHERE end_time < NOW() - INTERVAL '30 days'`

## Sources

### Primary (HIGH confidence)
- [Supabase Edge Functions Documentation](https://supabase.com/docs/guides/functions) - Official Deno runtime specs
- [Scheduling Edge Functions](https://supabase.com/docs/guides/functions/schedule-functions) - pg_cron + Edge Function pattern
- [Edge Functions Limits](https://supabase.com/docs/guides/functions/limits) - Free tier: 500K invocations, 150s timeout, 256MB
- [Supabase Cron Documentation](https://supabase.com/docs/guides/cron) - pg_cron setup and usage
- [Supabase JavaScript Upsert](https://supabase.com/docs/reference/javascript/upsert) - onConflict behavior
- [Row Level Security](https://supabase.com/docs/guides/database/postgres/row-level-security) - Service role bypass

### Secondary (MEDIUM confidence)
- [Processing Large Jobs with Edge Functions](https://supabase.com/blog/processing-large-jobs-with-edge-functions) - Job queue pattern
- [Axios vs Fetch 2026 Comparison](https://iproyal.com/blog/axios-vs-fetch/) - Deno incompatibility with axios
- [Node.js Retry Logic Patterns](https://v-checha.medium.com/advanced-node-js-patterns-implementing-robust-retry-logic-656cf70f8ee9) - Exponential backoff implementation
- [Supabase RLS Troubleshooting](https://supabase.com/docs/guides/troubleshooting/why-is-my-service-role-key-client-getting-rls-errors-or-not-returning-data-7_1K9z) - Service role key usage
- [Prevent Supabase Free Tier Pausing](https://shadhujan.medium.com/how-to-keep-supabase-free-tier-projects-active-d60fd4a17263) - 7-day inactivity rule

### Tertiary (LOW confidence - general best practices)
- [Database Caching Strategies](https://docs.aws.amazon.com/whitepapers/latest/database-caching-strategies-using-redis/caching-patterns.html) - Cache-aside pattern concepts
- [Backend Engineering Best Practices 2026](https://www.refontelearning.com/blog/backend-engineering-in-2026-top-tools-best-practices-and-career-insights) - Caching layer importance
- [API Rate Limiting Guide 2026](https://www.levo.ai/resources/blogs/api-rate-limiting-guide-2026) - General rate limit handling

## Metadata

**Confidence breakdown:**
- Standard stack: HIGH - Supabase Edge Functions and pg_cron verified via official documentation, free tier limits confirmed
- Architecture: HIGH - Patterns extracted from official Supabase examples and documentation
- Pitfalls: HIGH - RLS behavior, upsert mechanics, and cron issues verified in official docs and GitHub discussions
- JSearch API specifics: MEDIUM - Rate limits from project context, exact behavior unverified with official JSearch docs
- Local development cron: MEDIUM - Known limitation documented in community discussions, not official troubleshooting guide

**Research date:** 2026-02-02
**Valid until:** 2026-03-02 (30 days - stable technologies, but Supabase actively developing features)
